{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Isak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, json\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    return tweets        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trump_tweets.json comes from the Trump Twitter collection here (downloaded 1/19/19)\n",
    "http://www.trumptwitterarchive.com/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/trump_tweets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=[]\n",
    "for tweet in tweets:\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "# The order here is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-zâ€™'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet number 1\n",
      "Mexico  is  doing  NOTHING  to  stop  the  Caravan  which  is  now  fully  formed  and  heading  to  the  United  States.  We  stopped  the  last  two  -  many  are  still  in  Mexico  but  canâ€™t  get  through  our  Wall,  but  it  takes  a  lot  of  Border  Agents  if  there  is  no  Wall.  Not  easy!  \n",
      "\n",
      "Mexico  is  doing  NOTHING  to  stop  the  Caravan  which  is  now  fully  formed  and  heading  to  the  United  States  .  We  stopped  the  last  two  -  many  are  still  in  Mexico  but  can  â€™  t  get  through  our  Wall  ,  but  it  takes  a  lot  of  Border  Agents  if  there  is  no  Wall  .  Not  easy  !  \n",
      "\n",
      "Mexico  is  doing  NOTHING  to  stop  the  Caravan  which  is  now  fully  formed  and  heading  to  the  United  States  .  We  stopped  the  last  two  -  many  are  still  in  Mexico  but  can  â€™  t  get  through  our  Wall  ,  but  it  takes  a  lot  of  Border  Agents  if  there  is  no  Wall  .  Not  easy  !  \n",
      "\n",
      "Mexico  is  doing  NOTHING  to  stop  the  Caravan  which  is  now  fully  formed  and  heading  to  the  United  States  .  We  stopped  the  last  two  -  many  are  still  in  Mexico  but  ca  nâ€™t  get  through  our  Wall  ,  but  it  takes  a  lot  of  Border  Agents  if  there  is  no  Wall  .  Not  easy  !  \n",
      "\n",
      "Mexico  is  doing  NOTHING  to  stop  the  Caravan  which  is  now  fully  formed  and  heading  to  the  United  States  .  We  stopped  the  last  two  -  many  are  still  in  Mexico  but  canâ€™t  get  through  our  Wall  ,  but  it  takes  a  lot  of  Border  Agents  if  there  is  no  Wall  .  Not  easy  !  \n",
      "\n",
      "\n",
      "Tweet number 2\n",
      "Many  people  are  saying  that  the  Mainstream  Media  will  have  a  very  hard  time  restoring  credibility  because  of  the  way  they  have  treated  me  over  the  past  3  years  (including  the  election  lead-up),  as  highlighted  by  the  disgraceful  Buzzfeed  story  &amp;  the  even  more  disgraceful  coverage!  \n",
      "\n",
      "Many  people  are  saying  that  the  Mainstream  Media  will  have  a  very  hard  time  restoring  credibility  because  of  the  way  they  have  treated  me  over  the  past  3  years  (  including  the  election  lead-up  )  ,  as  highlighted  by  the  disgraceful  Buzzfeed  story  &  amp  ;  the  even  more  disgraceful  coverage  !  \n",
      "\n",
      "Many  people  are  saying  that  the  Mainstream  Media  will  have  a  very  hard  time  restoring  credibility  because  of  the  way  they  have  treated  me  over  the  past  3  years  (  including  the  election  lead-up  )  ,  as  highlighted  by  the  disgraceful  Buzzfeed  story  &  the  even  more  disgraceful  coverage  !  \n",
      "\n",
      "Many  people  are  saying  that  the  Mainstream  Media  will  have  a  very  hard  time  restoring  credibility  because  of  the  way  they  have  treated  me  over  the  past  3  years  (  including  the  election  lead  -  up  )  ,  as  highlighted  by  the  disgraceful  Buzzfeed  story  &  amp  ;  the  even  more  disgraceful  coverage  !  \n",
      "\n",
      "Many  people  are  saying  that  the  Mainstream  Media  will  have  a  very  hard  time  restoring  credibility  because  of  the  way  they  have  treated  me  over  the  past  3  years  (  including  the  election  lead-up  )  ,  as  highlighted  by  the  disgraceful  Buzzfeed  story  &  amp  ;  the  even  more  disgraceful  coverage  !  \n",
      "\n",
      "\n",
      "Tweet number 3\n",
      "The  Economy  is  one  of  the  best  in  our  history,  with  unemployment  at  a  50  year  low,  and  the  Stock  Market  ready  to  again  break  a  record  (set  by  us  many  times)  -  &amp;  all  you  heard  yesterday,  based  on  a  phony  story,  was  Impeachment.  You  want  to  see  a  Stock  Market  Crash,  Impeach  Trump!  \n",
      "\n",
      "The  Economy  is  one  of  the  best  in  our  history  ,  with  unemployment  at  a  50  year  low  ,  and  the  Stock  Market  ready  to  again  break  a  record  (  set  by  us  many  times  )  -  &  amp  ;  all  you  heard  yesterday  ,  based  on  a  phony  story  ,  was  Impeachment  .  You  want  to  see  a  Stock  Market  Crash  ,  Impeach  Trump  !  \n",
      "\n",
      "The  Economy  is  one  of  the  best  in  our  history  ,  with  unemployment  at  a  50  year  low  ,  and  the  Stock  Market  ready  to  again  break  a  record  (  set  by  us  many  times  )  -  &  all  you  heard  yesterday  ,  based  on  a  phony  story  ,  was  Impeachment  .  You  want  to  see  a  Stock  Market  Crash  ,  Impeach  Trump  !  \n",
      "\n",
      "The  Economy  is  one  of  the  best  in  our  history  ,  with  unemployment  at  a  50  year  low  ,  and  the  Stock  Market  ready  to  again  break  a  record  (  set  by  us  many  times  )  -  &  amp  ;  all  you  heard  yesterday  ,  based  on  a  phony  story  ,  was  Impeachment  .  You  want  to  see  a  Stock  Market  Crash  ,  Impeach  Trump  !  \n",
      "\n",
      "The  Economy  is  one  of  the  best  in  our  history  ,  with  unemployment  at  a  50  year  low  ,  and  the  Stock  Market  ready  to  again  break  a  record  (  set  by  us  many  times  )  -  &  amp  ;  all  you  heard  yesterday  ,  based  on  a  phony  story  ,  was  Impeachment  .  You  want  to  see  a  Stock  Market  Crash  ,  Impeach  Trump  !  \n",
      "\n",
      "\n",
      "Tweet number 4\n",
      ".@newtgingrich  just  stated  that  there  has  been  no  president  since  Abraham  Lincoln  who  has  been  treated  worse  or  more  unfairly  by  the  media  than  your  favorite  President,  me!  At  the  same  time  there  has  been  no  president  who  has  accomplished  more  in  his  first  two  years  in  office!  \n",
      "\n",
      ".  @  newtgingrich  just  stated  that  there  has  been  no  president  since  Abraham  Lincoln  who  has  been  treated  worse  or  more  unfairly  by  the  media  than  your  favorite  President  ,  me  !  At  the  same  time  there  has  been  no  president  who  has  accomplished  more  in  his  first  two  years  in  office  !  \n",
      "\n",
      ".  @newtgingrich  just  stated  that  there  has  been  no  president  since  Abraham  Lincoln  who  has  been  treated  worse  or  more  unfairly  by  the  media  than  your  favorite  President  ,  me  !  At  the  same  time  there  has  been  no  president  who  has  accomplished  more  in  his  first  two  years  in  office  !  \n",
      "\n",
      ".@newtgingrich  just  stated  that  there  has  been  no  president  since  Abraham  Lincoln  who  has  been  treated  worse  or  more  unfairly  by  the  media  than  your  favorite  President  ,  me  !  At  the  same  time  there  has  been  no  president  who  has  accomplished  more  in  his  first  two  years  in  office  !  \n",
      "\n",
      ".  @newtgingrich  just  stated  that  there  has  been  no  president  since  Abraham  Lincoln  who  has  been  treated  worse  or  more  unfairly  by  the  media  than  your  favorite  President  ,  me  !  At  the  same  time  there  has  been  no  president  who  has  accomplished  more  in  his  first  two  years  in  office  !  \n",
      "\n",
      "\n",
      "Tweet number 5\n",
      "Will  be  leaving  for  Dover  to  be  with  the  families  of  4  very  special  people  who  lost  their  lives  in  service  to  our  Country!  \n",
      "\n",
      "Will  be  leaving  for  Dover  to  be  with  the  families  of  4  very  special  people  who  lost  their  lives  in  service  to  our  Country  !  \n",
      "\n",
      "Will  be  leaving  for  Dover  to  be  with  the  families  of  4  very  special  people  who  lost  their  lives  in  service  to  our  Country  !  \n",
      "\n",
      "Will  be  leaving  for  Dover  to  be  with  the  families  of  4  very  special  people  who  lost  their  lives  in  service  to  our  Country  !  \n",
      "\n",
      "Will  be  leaving  for  Dover  to  be  with  the  families  of  4  very  special  people  who  lost  their  lives  in  service  to  our  Country  !  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [whitespace_tokens, nltk_tokens, nltk_casual_tokens, spacy_tokens, extensible_tokens]       \n",
    "for tweet in range(5):\n",
    "    print(\"\\nTweet number\", tweet+1)\n",
    "    for tokenizer in tokenizers:\n",
    "        for word in tokenizer[tweet]:\n",
    "            print(word, ' ', end='')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compare(tokenization_one, tokenization_two):\n",
    "    # First will flatten out both tokenizations into lists\n",
    "    flat_one, flat_two = [], []\n",
    "    for tweet in tokenization_one:\n",
    "        for word in tweet:\n",
    "            flat_one.append(word)\n",
    "    for tweet in tokenization_two:\n",
    "        for word in tweet:\n",
    "            flat_two.append(word)\n",
    "    \n",
    "    # Then check for occurences across lists\n",
    "    unique = []\n",
    "    wordcount = 0;\n",
    "    for word in flat_one:\n",
    "        wordcount += 1\n",
    "        if word not in flat_two:\n",
    "            unique.append(word)\n",
    "\n",
    "    # Found documentation on collections.Counter at https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "    occurence = Counter(unique)\n",
    "    print(occurence.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"', 24807), ('@realDonaldTrump', 8661), ('#Trump2016', 840), ('@BarackObama', 732), (\"don't\", 626), ('#MakeAmericaGreatAgain', 560), ('@FoxNews', 547), (\"I'm\", 524), ('@foxandfriends', 504), (\"can't\", 423), ('@ApprenticeNBC', 393), ('@MittRomney', 314), (\"It's\", 304), (\"it's\", 303), ('ðŸ‡º', 300), ('ðŸ‡¸', 300), ('#CelebApprentice', 289), ('@CNN', 285), (\"you're\", 276), (\"doesn't\", 266)]\n"
     ]
    }
   ],
   "source": [
    "compare(nltk_casual_tokens, nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 57646 \n",
      "Average sentence length is 11\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "sentence_lengths = []\n",
    "sentence_number = 0\n",
    "end_signs = ['.','!','?']\n",
    "for tweet in nltk_tokens:\n",
    "    counter = 0;\n",
    "    for word in tweet:\n",
    "        if word not in end_signs:\n",
    "            counter += 1\n",
    "        else:\n",
    "            sentence_lengths.append(counter)\n",
    "            sentence_number += 1\n",
    "            counter = 0\n",
    "avg = round(mean(sentence_lengths))\n",
    "print(\"Number of sentences:\", sentence_number,\n",
    "      \"\\nAverage sentence length is\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "# FILL IN HERE\n",
    "# Documentation found at https://www.w3schools.com/python/python_regex.asp\n",
    "r\"(?:http[s]?://(?:[\\w_]|[$-_@.&+~]|[!*\\(\\),])+)\",\n",
    "         \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-zâ€™'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "course\n",
      "website\n",
      "is\n",
      "http://people.ischool.berkeley.edu/~dbamman/info256.html\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
